\documentclass[../Main.tex]{subfiles}

\begin{document}

\begin{justify}
En este capítulo se presenta toda la información recolectada y considerada como relevante en relación con la generación de texto y la generación de respuesta a preguntas.
\end{justify}

\section{Introducción}
\begin{justify}
Se ha elaborado una tabla que consiste en cincuenta (50) documentos, principalmente artículos de los cuales se ha recolectado información relevante de cada uno con el fin de identificar las técnicas y algoritmos usados en investigaciones, desarrollo de modelos/sistemas, implementaciones u otros.
\end{justify}

\begin{justify}
La elección de documentos se ha dividido en dos tipos de tareas principales de Procesamiento de Lenguaje Natural (PLN), la primera generación de texto (Text Generation) y la segunda generación de respuestas a preguntas (Question Answering), veinticuatro y veintiséis documentos respectivamente.
\end{justify}

\begin{justify}
Los documentos seleccionados en su mayoría son del año 2008 hasta el presente, esta consideración se tuvo debido al acelerado avance en el campo de NLP hecho por investigadores, con ello el uso de técnicas más sofisticadas y mejores resultados en experimentos realizados.
\end{justify}

\begin{justify}
La información recolectada e identificada como relevante para el trabajo actual, será fundamental para la implementación del modelo adaptado que generará exámenes de Inglés, a continuación se adjunta la tabla elaborada y posteriormente su análisis.
\end{justify}

\subsection{Documentos recolectados}
\begin{justify}
En esta sección se presenta una parte de los cincuenta (50) documentos recolectados, se tuvo en cuenta información relevante como la siguiente:

\begin{itemize}
	\item Técnicas de Procesamiento de Lenguaje Natural\par

	\item Ventajas\par

	\item Desventajas \par

    \item Logros \par
    
    \item Trabajos futuros \par
    
    \item Otros \par
\end{itemize}\par
\end{justify}

\par

\begin{table}[H]
\begin{Center}
	\includegraphics[width=6.4in,height=3.4in]{Images/InicioTablaPapers.png}
\caption{Tabla documentos recolectados}
Fuente: Elaboración propia
\label{tab:table1}
\end{Center}
 \end{table}

La tabla completa se encuentra en los anexos.

\subsection{Algoritmos recolectados}
\begin{justify}
En esta sección se presenta una parte de los diez (10) algoritmos recolectados, se tuvo en cuenta información relevante de manera similar al ítem 3.1.1.
\end{justify}

\begin{table}[H]
\begin{Center}
	\includegraphics[width=6.4in,height=3.4in]{Images/InicioTablaAlgoritmos.png}
    \caption{Algoritmos recolectados}
    Fuente: Elaboración propia
    \label{fig:section}
\label{tab:table1}
\end{Center}
\end{table}

La tabla completa se encuentra en los anexos.

%\newgeometry{left=0in,right=0in,top=0in,bottom=0.25in}
%\begin{landscape}
%\newpage
%\tiny{
%\subfile{../../Tables/tablaPapers}
%}
%\end{landscape}
%\restoregeometry % restores the margins after frontpage

\subsection{Análisis}

\begin{justify}
De acuerdo a la relevancia de cada documento se le ha asignado un valor de uno (1) a cinco (5) a consideración basándose en ventajas, desventajas, logros, entre otros, siendo cinco (5) muy relevante y uno (1) poco relevante. De acuerdo con el año de publicación se evidencia que después de haber realizado la selección de documentos, los publicados recientemente son los más relevantes en cuanto a resultados obtenidos.\par
\end{justify}

\begin{figure}[H]
	\begin{Center}
		\includegraphics[width=6in,height=3.3in]{Images/GraficoPapers.png}
	    \caption{Relevancia documentos recolectados}
	    Fuente: Elaboración propia
        \label{fig:section}
	\end{Center}
\end{figure}

\begin{justify}
Entre los más relevantes se encuentra BERT (Bidirectional Encoder Representations from Transformers), una técnica de aprendizaje automático basada en Transformer para la formación previa del Procesamiento del Lenguaje Natural (PLN) desarrollada por Google. Se puede evidenciar que la técnica BERT está presente en los documentos considerados más relevantes y en una cantidad importante de ellos, esto se debe a que los resultados obtenidos en los experimentos realizados por los autores de los artículos son prometedores.
\end{justify}

\begin{justify}
Del mismo modo GPT-2 (Generative Pre-trained Transformer 2), una inteligencia artificial de código abierto creada por OpenAI que  utiliza el aprendizaje profundo para diferentes tareas (Traducción, responder preguntas, resumir pasajes y generar resultados de texto), se encuentra citada en repetidas ocasiones, siendo el más prometedor entre los diferentes tipos de modelos propuestos en los artículos. Su sucesor GPT-3 no ha sido considerado como opción viable debido al acceso restringido a la API y al objetivo al que está enfocado su uso.
\end{justify}


\begin{justify}
En cada documento analizado se logró recopilar las diferentes técnicas de Procesamiento de Lenguaje Natural con el fin de conocer en detalle la construcción de los modelos propuestos o investigaciones presentadas (teoría). Por tanto a continuación se describen las técnicas de PLN utilizadas según la información recopilada en la tabla para las tareas de generación de texto y respuesta a preguntas.
\end{justify}

\begin{itemize}
	\item \textbf{Tokenization:} La tokenización consiste esencialmente en dividir una frase, oración, párrafo o un documento de texto completo en unidades más pequeñas, como palabras o términos individuales. Cada una de estas unidades más pequeñas se llama tokens \cite{34}.\par
	
	\item \textbf{Normalization:} Un paso de preprocesamiento que se pasa por alto es la normalización del texto. La normalización de texto es el proceso de transformar un texto en una forma canónica (estándar). Por ejemplo, la palabra "gooood" y "gud" se puede transformar en "good", su forma canónica \cite{35}. \par

	\item \textbf{StopWords Pre-Processing:} Las palabras vacías son un conjunto de palabras de uso común en un idioma. Ejemplos de palabras vacías en inglés son "a", "the", "is", "are", etc. La intuición detrás del uso de palabras vacías es que, al eliminar las palabras con poca información del texto y así centrarse en las palabras importantes \cite{35}. \par

    \item \textbf{Stemming:} Es el proceso de reducir la inflexión de las palabras (ej. troubled, troubles) a su forma raíz  (ej. trouble). La "raíz" en este caso puede no ser una palabra raíz real, sino simplemente una forma canónica de la palabra original \cite{35}. \par
    
    \item \textbf{Lemmatization:} La lematización en la superficie es muy similar al stemming, donde el objetivo es eliminar las inflexiones y asignar una palabra a su forma raíz. La única diferencia es que la lematización intenta hacerlo de la manera adecuada. No solo corta cosas, en realidad transforma las palabras en la raíz real \cite{35}. \par
    
    \item \textbf{StopWords Pre-Processing:} Palabras vacías es el nombre que reciben las palabras sin significado como artículos, pronombres, preposiciones, etc. que son filtradas antes o después del procesamiento de datos en lenguaje natural \cite{35}. \par
    
    \item \textbf{PoS Tagging (part-of-speech tagging):} Es el proceso de marcar una palabra en un texto (corpus) como correspondiente a una parte particular del discurso, basado tanto en su definición como en su contexto \cite{36}. \par

    \item \textbf{Term Frequency:} Frecuencia de término (TF) significa la frecuencia con la que aparece un término en un documento. En el contexto del lenguaje natural, los términos corresponden a palabras o frases. Pero los términos también pueden representar cualquier símbolo en el texto. Se trata de cómo lo defines. La frecuencia de término se usa comúnmente en tareas de minería de textos, aprendizaje automático y recuperación de información \cite{37}. \par
    
    % https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4
    \item \textbf{Word Embedding:} La incrustación de palabras es una técnica en la que las palabras individuales de un dominio o idioma se representan como vectores de valor real en un espacio dimensional inferior \cite{38}. \par
    
    %https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4
    \item \textbf{Bag of words:} La bolsa de palabras es una simple y popular técnica de extracción de características a partir del texto. El modelo de bolsa de palabras procesa el texto para encontrar cuántas veces apareció cada palabra en la oración. Esto también se llama vectorización \cite{38}. \par
    
    %https://keras.io/examples/nlp/masked_language_modeling/
    \item \textbf{Masked LM (MLM):} El modelado de lenguaje enmascarado es una tarea de rellenar espacios en blanco, en la que un modelo usa las palabras de contexto que rodean un símbolo de máscara para intentar predecir cuál debería ser la palabra enmascarada \cite{39}. \par
    
    
    % https://www.kdnuggets.com/2018/08/named-entity-recognition-practitioners-guide-nlp-4.html
    \item \textbf{Named Entity Recognition (NER):} El reconocimiento de entidad con nombre (NER) (también conocido como identificación de entidad (con nombre), fragmentación de entidad y extracción de entidad) es una técnica popular utilizada en la extracción de información para identificar y segmentar las entidades nombradas y clasificarlas o categorizarlas en varias clases predefinidas \cite{40}.
\end{itemize}\par


\begin{justify}
Para la generación de preguntas en la mayoría de modelos se utiliza SQuAD (Stanford Question Answering Dataset) para entrenar y evaluar los modelos. SQuAD es un conjunto de datos de comprensión de lectura, que consta de preguntas planteadas por los trabajadores de la red en un conjunto de artículos de Wikipedia, donde la respuesta a cada pregunta es un segmento de texto, o un intervalo, del pasaje de lectura correspondiente, o la pregunta podría ser incontestable \cite{41}.
\end{justify}

\begin{justify}
Gracias a la revisión del estado del arte y recolección de información presentada en la tabla es posible tener una base sólida en la elección de técnicas y modelos a utilizar en el trabajo de grado actual (generación de exámenes de Inglés).
\end{justify}

\begin{justify}
Adicionalmente, una vez obtenida las fuentes teóricas, se recopiló información de manera más específica sobre algoritmos que se basan en la teoría presente en la mayoría de documentos, para este propósito también se realiza una tabla asociada a 10 algoritmos de código abierto disponible en repositorios o plataformas reconocidas como Hugging Face \cite{42}.
\end{justify}

\begin{justify}
Después de realizar una búsqueda exhaustiva de algoritmos de código libre disponibles en la Web que fueran potencialmente útiles para cumplir el primer objetivo propuesto, se puede concluir que, los modelos basados en Transformers ofrecen resultados de vanguardia tanto para la tarea de generación de texto como para la tarea de respuesta a preguntas. 
\end{justify}

\subsection{Resultados}
\begin{justify}
Tras realizar un análisis profundo sobre los artículos y algoritmos recolectados se describe a continuación los algoritmos seleccionados para la implementación en el prototipo web:

\begin{itemize}
    \item \textbf{Generación de texto}\\
    Para el algoritmo de generación de texto se seleccionó GPT-2 de Hugging Face \cite{43} principalmente por la diferencia de rendimiento en comparación con los otros algoritmos recopilados. Dicha diferencia representa casi el doble de tiempo de ejecución en igualdad de condiciones (entradas). Dado que la comparación se redujo a dos algoritmos, el de Hugging Face (GPT-2) y un algoritmo encontrado en GitHub llamado GPT2-Pytorch \cite{44}, ambos están claramente basados en GPT-2 lo cual implica que los textos generados sean similares al modelo original GPT-2 y que la diferencia se defina por el rendimiento de generación de texto.
    
    El algoritmo proporcionado por Hugging Face es oficial respecto a GPT2-Pytorch que es una modificación de GPT-2 hecha por un desarrollador de la comunidad, de allí las diferencias en algunos aspectos como la calidad de los resultados, esto se debe a que, Hugging Face es un proveedor de código abierto reconocido en el campo de Procesamiento de Lenguaje Natural (PLN).
    
    Se realizaron encuestas a un docente de Inglés con el fin de obtener una evaluación de un experto en cuanto a la calidad de los textos generados por ambos algoritmos, se compararon tiempos de respuesta para comparar el rendimiento, se tuvieron en cuenta las limitaciones y otros detalles, finalmente, los resultados obtenidos favorecieron a GPT-2 Hugging Face.
    
    \item \textbf{Generación de preguntas y respuestas}\\
    Para la selección del algoritmo de generación de preguntas y respuestas, se compararon dos algoritmos de código abierto para esta tarea, sin embargo, desde un inicio uno de los algoritmos presentaba mayor potencial de acuerdo al propósito del proyecto, ya que, de los recolectados es el único algoritmo capaz de generar preguntas y respuestas de opción múltiple y de oraciones (preguntas abiertas), es decir dos tipos de preguntas.
    
    El tipo de preguntas es un factor importante para el propósito del proyecto, ya que, a mayor tipo de preguntas más posibilidades de generar exámenes variados para los estudiantes.
    
    De acuerdo a las pruebas y posterior análisis realizado entre ambos algoritmos inicialmente seleccionados, se decidió seleccionar finalmente el algoritmo encontrado en GitHub llamado Question Generator \cite{45}, debido a todas las ventajas obtenidas respecto a el propósito del proyecto expuestas anteriormente.
\end{itemize}

    La justificación de selección de algoritmos completa se encuentra en los anexos.
\end{justify}
\end{document}